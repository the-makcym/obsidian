|            variable            |              stands for               |
|:------------------------------:|:-------------------------------------:|
|              $x$               |             given column              |
|              $y$               |            obtained column            |
|              $t$               |             target column             |
|              $s$               |            size of dataset            |
|              $l$               |           number of layers            |
|     $L = (L_1 \ldots L_l)$     |            sizes of layers            |
|              $z$               | input column for the particular layer |
| $w = (w^{(2)} \ldots w^{(l)})$ |      row of matrices of weights       |
|     $W = (W_1 \ldots W_k)$     |          row of all weights           |
| $b = (b^{(2)} \ldots b^{(l)})$ |       row of columns of biases        |
|     $B = (B_1 \ldots B_q)$     |           row of all biases           |
|             $\eta$             |             learning rate             |

___

$$
w^{(i)} = \begin{pmatrix}
w^{(i)}_{1,1} & \cdots & w^{(i)}_{1,p} \\  \\ 
\vdots & \ddots & \vdots \\
w^{(i)}_{n,1} & \cdots & w^{(i)}_{n,p} \\
\end{pmatrix}
$$
$$
b = (b^{(2)} \ldots b^{(l)}): \text{ }\\
b^{(i)} = \begin{pmatrix}
b^{(i)}_1 \\ \vdots \\ b^{(i)}_n
\end{pmatrix}
$$

---

Sigmoid logistic function:
$$
\sigma (z) = \frac{1}{1+e^{-z}}
$$
$$
\sigma '(z) = \frac{1}{1+e^{-z}} \cdot (1-\frac{1}{1+e^{-z}}) = \sigma(z) \cdot (1-\sigma (z))
$$

---

Column obtained from $L_i$:
$$
a^{(i)} = \begin{pmatrix}
a^{(i)}_1 \\ \vdots \\ a^{(i)}_{L_i}
\end{pmatrix}
$$
$$
a^{(i)}=\sigma( w^{(i)} \cdot a^{(i-1)} + b^{(i)} ) 
$$
$$
a^{(1)} = x 
$$
$$
y=a^{(l)}
$$

---

Cost function (also known as mean squared error - MSE):
$$
C(w, b) = \frac{1}{2s} \sum_x |t-y|^2 = \frac{1}{2s} \sum_x |t-y(w, b)|^2
$$

$$
\nabla C_W = (\frac{\partial C}{\partial W_1} \ldots \frac{\partial C}{\partial W_k})
\; , \:
\nabla C_B = (\frac{\partial C}{\partial B_1} \ldots \frac{\partial C}{\partial B_q})
$$
$$

\Delta W = (\Delta W_1 \ldots \Delta W_k)
\; , \:
\Delta B = (\Delta B_1 \ldots \Delta B_q)\\
$$

$$
\Delta C \approx (\nabla C_W \space,\space \Delta W) + (\nabla C_B \space,\space \Delta B) 
$$
take:
$$
\Delta W = -\eta \cdot \nabla C_W
\; , \;
\Delta B = -\eta \cdot \nabla C_B 
$$
then:
$$
\Delta C \approx -\eta \cdot (|\nabla C_W|^2+|\nabla C_B|^2) < 0
$$

---

### Backpropagation

Computing $\nabla C_W,\; \nabla C_B$  for single $x$:

$$
C = \frac12 \cdot |t-y|^2 = \frac12 \cdot \sum_{i=1}^{L_l} (t_i - a^{(l)}_i)^2
$$

Error function related to the $j$ neuron $L_i$:

$$
\delta^{(i)}_j = \frac{\partial C}{\partial z^{(i)}_j}
$$
$$
\delta^{(i)} = \begin{pmatrix}
\delta^{(i)}_1 \\ \vdots \\ \delta^{(i)}_{L_i}
\end{pmatrix}
$$

---

**BP1**

Since $a^{(i)}_j = \sigma(z^{(i)}_j)$:
$$
\delta^{(i)}_j  = \frac{\partial C}{\partial a^{(i)}_j} \cdot \sigma '(z^{(i)}_j)= \frac{\partial C}{\partial z^{(i)}_j}
$$

For the last layer $l$:
$$
\delta^{(l)} = (a^{(l)}-t) \odot \sigma'(z^{(l)})
$$

---

**BP2**

$$
\delta^{(i)}=\Big((w^{(i+1)})^T \cdot \delta^{(i+1)}\Big) \odot \sigma'(z^{(i)})
$$

---

**BP3**

$$
\frac{\partial C}{\partial b^{(i)}} = \delta^{(i)}
$$

---

**BP4**

$$
\frac{\partial C}{\partial w^{(i)}_{n,p}} = a^{(i-1)}_p \cdot \delta^{(i)}_n
$$
$$
\frac{\partial C}{\partial w^{(i)}} = \delta^{(i)} \cdot (a^{(i-1)})^T
$$

$$
x=\begin{pmatrix}
x_1 \\ \vdots \\ x_p
\end{pmatrix},\;
z=\begin{pmatrix}
z_1 \\ \vdots \\ z_n
\end{pmatrix},\;
w = \begin{pmatrix}
w_{1,1} & \cdots & w_{1,p} \\
\vdots & \ddots & \vdots \\
w_{n,1} & \cdots & w_{n,p}
\end{pmatrix},\;
b=\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}
$$

Input in hidden layer:

$$
z=\frac1{1+e^{-w\cdot x - b}}
$$